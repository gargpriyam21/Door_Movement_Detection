{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2130e8",
   "metadata": {},
   "source": [
    "# Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48679b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import *\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791745d6",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f93263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data directory\n",
    "data_path = os.getcwd() + '/IoT_ASN4_Group11/data/'\n",
    "\n",
    "# Path to the train and the test data\n",
    "train_data_file = data_path + 'train_data.txt'\n",
    "test_data_file = data_path + 'test_data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ded30",
   "metadata": {},
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2093ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_file):\n",
    "    \"\"\"\n",
    "    Function to process the given data and spliit it in the respective required columns\n",
    "    Arguments:\n",
    "        data_file: Provided data in the given format\n",
    "    \"\"\"\n",
    "    label_map = {'CLOSED': 0, 'OPEN': 1, 'STATIONARY': 2}\n",
    "    \n",
    "    processed_data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            content = line.split(',')\n",
    "            ax = content[0].split(\" \")[2][1:]\n",
    "            gz = content[5][:-1]\n",
    "            ay,az,gx,gy = content[1:5]\n",
    "            label = str(label_map[content[-1][:-1].strip()])\n",
    "            train_sample = [ax,ay,az,gx,gy,gx,label]\n",
    "\n",
    "            processed_data.append(list(map(lambda x: float(x.strip()), train_sample)))\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5afce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stationary_values(processed_data):\n",
    "    \"\"\"\n",
    "    Function to remove the Stationary values from the data i.e. not required for the training of the model\n",
    "    Arguments:\n",
    "        processed_data: Processed data in the form of 2D array\n",
    "    \"\"\"\n",
    "    data = collections.defaultdict(list)\n",
    "    start = False\n",
    "    for observation in processed_data:\n",
    "        label =  observation[-1] \n",
    "        if label != 2.0:\n",
    "            if not start:\n",
    "                start = True\n",
    "                data[label].append([])\n",
    "            data[label][-1].append(observation[:-1])\n",
    "        else:\n",
    "            start = False\n",
    "            continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unncessary_values(data):\n",
    "    \"\"\"\n",
    "    Function to remove the Unnecessary values and the noise from the data\n",
    "    Arguments:\n",
    "        data: data in the form of 2D array\n",
    "    \"\"\"\n",
    "    for key, value in data.items():\n",
    "        rem_index = []\n",
    "        for i, observation in enumerate(value):\n",
    "    #         print(len(observation))\n",
    "            if len(observation) < 5:\n",
    "                rem_index.append(i)\n",
    "        data[key] = delete_multiple_element(data[key], rem_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_multiple_element(list_object, indices):\n",
    "    \"\"\"\n",
    "    Function to delete the multiple elements present in the data\n",
    "    Arguments:\n",
    "        list_object: list of object\n",
    "        indices: indices to be deleted\n",
    "    \"\"\"\n",
    "    indices = sorted(indices, reverse=True)\n",
    "    for idx in indices:\n",
    "        if idx < len(list_object):\n",
    "            list_object.pop(idx)\n",
    "    return list_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_and_labels(data_file):\n",
    "    \"\"\"\n",
    "    Function to create the samples and the labels from the given data file\n",
    "    Arguments:\n",
    "        data_file: path to the file\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_data = process_data(data_file)\n",
    "    data = remove_stationary_values(processed_data)\n",
    "    remove_unncessary_values(data)\n",
    "\n",
    "    samples =  data[1.0] + data[0.0]\n",
    "    labels = [1]*len(data[1.0]) + [0]*len(data[0.0])\n",
    "    \n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the samples and the labesl for the training data\n",
    "samples, labels = get_samples_and_labels(train_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a825527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collections.Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428ac26",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85de7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature1(window):\n",
    "    \"\"\"\n",
    "    Function to fetch 3 featured from the given window\n",
    "        Feature 1: Mean of Accelerometer x\n",
    "        Feature 2: Mean of Accelerometer y\n",
    "        Feature 3: Mean of Gyroscope z\n",
    "    Arguments:\n",
    "        window: window for the feature extraction\n",
    "    \"\"\"\n",
    "    if len(window) == 0:\n",
    "        return None\n",
    "    feature = []\n",
    "    mean_values = np.mean(window, axis=0)\n",
    "    \n",
    "    feature.append(mean_values[0])\n",
    "    feature.append(mean_values[1])\n",
    "    feature.append(mean_values[5])\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def feature2(window):\n",
    "    \"\"\"\n",
    "    Function to fetch 4 featured from the given window\n",
    "        Feature 1: Mean of Accelerometer Values\n",
    "        Feature 2: Mean of Gyroscope Values\n",
    "        Feature 3: Standard Deviation of Accelerometer Values\n",
    "        Feature 4: Standard Deviation of Gyroscope Values\n",
    "    Arguments:\n",
    "        window: window for the feature extraction\n",
    "    \"\"\"\n",
    "    feature = []\n",
    "    \n",
    "    feature.append(np.mean(window[:,0:3]))\n",
    "    feature.append(np.mean(window[:,3:6]))\n",
    "    feature.append(np.std(window[:,0:3]))\n",
    "    feature.append(np.std(window[:3:6]))\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_from_window(window):\n",
    "    \"\"\"\n",
    "    Function fetch main features from the given window\n",
    "    Arguments:\n",
    "        window: window for the feature extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    f = feature1(window)\n",
    "#     f = feature2(window)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc64667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vector(sample, splits):\n",
    "    \"\"\"\n",
    "    Function to create the feature vector from the given data and creating the features\n",
    "    Arguments:\n",
    "        sample: sample data for the feature extraction\n",
    "        splits: number of splits in which the data is to be splitted\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vector = []\n",
    "    if len(sample) < splits:\n",
    "#         feature_vector.append(select_features_from_window(np.array(sample)))\n",
    "        return feature_vector\n",
    "    to_split_data = np.array(sample[:int(len(sample)//splits)*splits])\n",
    "    splitted_samples = np.split(to_split_data,splits)\n",
    "    for i, window in enumerate(splitted_samples):\n",
    "        if i == len(splitted_samples)-1 and len(sample[int(len(sample)//splits)*splits:]) > 0: \n",
    "            window = np.vstack((window, sample[int(len(sample)//splits)*splits:]))\n",
    "        features = select_features_from_window(window)\n",
    "        if features:\n",
    "            feature_vector.append(features)\n",
    "\n",
    "    debug = ' '.join([str(len(x)) for x in feature_vector])\n",
    "#     print(len(feature_vector), \" \", debug)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094da1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(samples, splits):\n",
    "    \"\"\"\n",
    "    Function to create the data from the given samples into the required splits\n",
    "    Arguments:\n",
    "        samples: sample data for the feature extraction\n",
    "        splits: number of splits in which the data is to be splitted\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    i = 0\n",
    "    for sample in samples:\n",
    "        f_vector = create_feature_vector(sample, splits)\n",
    "        if f_vector != []:\n",
    "            data.append(f_vector)\n",
    "#     print(data)\n",
    "    data = np.array(data)\n",
    "#     print(data.shape)\n",
    "    data = data.reshape(data.shape[0], data.shape[1]*data.shape[2])\n",
    "#     print(data.shape)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b0785",
   "metadata": {},
   "source": [
    "# Model Identification, Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1609f",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform grid search with pca, c, gamma values\n",
    "def perform_gird_search(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Function to perform grid search with pca, c, gamma values\n",
    "    Arguments:\n",
    "        x_train: x values for the data values\n",
    "        y_train: labels values for the data values\n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        [('scaler', StandardScaler()),\n",
    "         ('pca', PCA()),\n",
    "         ('SVM', SVC(kernel='rbf', random_state=1234))]\n",
    "    )\n",
    "    \n",
    "    check_params= {\n",
    "        'pca__n_components': [2,4], \n",
    "        'SVM__C': [0.05,0.1,0.5, 1],\n",
    "        'SVM__gamma' : [0.1, 0.5]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(pipeline, check_params, refit=True, verbose=1, cv=3)\n",
    "    grid.fit(x_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    \n",
    "    return best_params, grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489fb701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning data for window size: 3\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "1.0\n",
      "Tuning data for window size: 4\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "num_window_to_best_params = {}\n",
    "\n",
    "for numw in range(3,5):\n",
    "    print(\"Tuning data for window size: {}\".format(numw))\n",
    "    \n",
    "    data = create_data(samples, numw)\n",
    "    data_shuffled, labels_shuffled = shuffle(data, labels, random_state=random_state_seed)\n",
    "    bparams, best_model = perform_gird_search(data_shuffled, labels_shuffled)\n",
    "    num_window_to_best_params[numw] = bparams\n",
    "    \n",
    "    print(best_model.score(data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b07699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Window Size: 3\n",
      "{'SVM__C': 0.05, 'SVM__gamma': 0.1, 'pca__n_components': 2}\n",
      "Best parameters for Window Size: 4\n",
      "{'SVM__C': 0.05, 'SVM__gamma': 0.1, 'pca__n_components': 2}\n"
     ]
    }
   ],
   "source": [
    "for window_size, parameters in num_window_to_best_params.items():\n",
    "    print(\"Best parameters for Window Size: {}\".format(window_size))\n",
    "    print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552acdb",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5958aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_window_size = 3\n",
    "\n",
    "best_pca__n_components = num_window_to_best_params[3][\"pca__n_components\"]\n",
    "best_SVM__C = num_window_to_best_params[3][\"SVM__C\"]\n",
    "best_SVM__gamma = num_window_to_best_params[3][\"SVM__gamma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0cca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=2)),\n",
       "                ('SVM', SVC(C=0.05, gamma=0.1, random_state=1234))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [('scaler', StandardScaler()),\n",
    "     ('pca', PCA(n_components = best_pca__n_components)),\n",
    "     ('SVM', SVC(kernel='rbf', random_state=random_state_seed,C = best_SVM__C, gamma=best_SVM__gamma,))]\n",
    ")\n",
    "\n",
    "data = create_data(samples, best_window_size)\n",
    "data_shuffled, labels_shuffled = shuffle(data, labels, random_state=random_state_seed)\n",
    "\n",
    "pipeline.fit(data_shuffled, labels_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0890aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline.score(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7f4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55,  0],\n",
       "       [ 0, 54]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix(pipeline.predict(data), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c469b09",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples, test_labels = get_samples_and_labels(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_data(test_samples, best_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea83265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shuffled_test_data, shuffled_test_labels = shuffle(test_data, test_labels, random_state = random_state_seed)\n",
    "pipeline.score(shuffled_test_data, shuffled_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d91e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 0],\n",
       "       [0, 6]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix(pipeline.predict(shuffled_test_data), shuffled_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563a850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e64f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
